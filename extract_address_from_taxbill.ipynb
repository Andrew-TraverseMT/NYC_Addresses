{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andrew-TraverseMT/NYC_Addresses/blob/main/extract_address_from_taxbill.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook takes records from MapPluto and looks up parcel owner mailing addresses from property tax assessments using the BBL."
      ],
      "metadata": {
        "id": "xQ1Dw6pfZCJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install PyPDF2\n",
        "!pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xEXs752KQRs",
        "outputId": "62405022-972a-4189-8990-110e951c7b98"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250327 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250327 pdfplumber-0.11.6 pypdfium2-4.30.1\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wWJ6U1OGKF_k"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pdfplumber\n",
        "import pymupdf\n",
        "import io\n",
        "import PyPDF2\n",
        "import re\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import warnings\n",
        "import logging\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_text_from_url(url):\n",
        "    \"\"\"\n",
        "    Extracts all selectable text from each page of a PDF accessible via a URL, handling redirects.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL pointing to the PDF (or a redirect to the PDF).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of strings, where each string contains the text from one page.\n",
        "              If no text is found on a page, a message is included for that page.\n",
        "              If an error occurs, a list with an error message is returned.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Download the PDF from the URL, following redirects\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}  # Add user-agent to avoid server blocks\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Check if the response content is a PDF\n",
        "        content_type = response.headers.get('Content-Type', '')\n",
        "        if 'application/pdf' not in content_type.lower():\n",
        "            return [\"Error: The URL does not point to a PDF file\"]\n",
        "\n",
        "        # Verify the content starts with %PDF to ensure it's a valid PDF\n",
        "        if not response.content.startswith(b'%PDF'):\n",
        "            return [\"Error: The response content is not a valid PDF\"]\n",
        "\n",
        "        # Try extracting text with pdfplumber\n",
        "        try:\n",
        "            with warnings.catch_warnings():  # Suppress CropBox warnings\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
        "                    all_text = []\n",
        "                    for page_number, page in enumerate(pdf.pages, start=1):\n",
        "                        text = page.extract_text()\n",
        "                        if text:\n",
        "                            # Clean the text by removing extra whitespace and empty lines\n",
        "                            cleaned_text = '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
        "                            all_text.append(f\"Page {page_number}:\\n{cleaned_text}\")\n",
        "                        else:\n",
        "                            all_text.append(f\"Page {page_number}: No selectable text found\")\n",
        "                    return all_text\n",
        "        except Exception as e:\n",
        "            # Fallback to PyPDF2 if pdfplumber fails\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(response.content))\n",
        "                all_text = []\n",
        "                for page_number, page in enumerate(pdf_reader.pages, start=1):\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        cleaned_text = '\\n'.join(line.strip() for line in text.split('\\n') if line.strip())\n",
        "                        all_text.append(f\"Page {page_number}:\\n{cleaned_text}\")\n",
        "                    else:\n",
        "                        all_text.append(f\"Page {page_number}: No selectable text found\")\n",
        "                return all_text\n",
        "            except Exception as fallback_e:\n",
        "                return [f\"Error extracting text: pdfplumber failed with '{e}', PyPDF2 failed with '{fallback_e}'\"]\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        return [f\"Error downloading PDF: {e}\"]\n",
        "    except Exception as e:\n",
        "        return [f\"Error processing PDF: {e}\"]\n",
        "\n",
        "# Example usage with your URL\n",
        "url = 'https://a836-edms.nyc.gov/dctm-rest/repositories/dofedmspts/StatementSearch?bbl=2023330031&stmtDate=20250215&stmtType=SOA'\n",
        "extracted_content = extract_all_text_from_url(url)\n",
        "\n",
        "# Print the extracted content for each page\n",
        "for page_content in extracted_content:\n",
        "    print(page_content)\n",
        "    print('-' * 50)  # Separator between pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMKEf45fCIQq",
        "outputId": "cb75a051-52aa-487d-e71d-dbc032ee11ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1:\n",
            "84016142502150100140001 NYNP\n",
            "Property Tax Bill Quarterly  Statement\n",
            "Activity through February 15, 2025\n",
            "Owner name: FAE HOLDINGS 395224R, LLC\n",
            "Property address: 289MORRIS AVENUE\n",
            "Amount Due 04/01/25: $0.00\n",
            "#840161425021501#\n",
            "FAE HOLDINGS 395224R, LLC\n",
            "C/0 CACTUS PROPERTIES 4 LLC\n",
            "129 BIRCH HILL RD.\n",
            "LOCUST VALLEY NY11560-18411400.01 -ZB -40 -2 -0 -4 -1561\n",
            "Borough: 2     Block: 02333     Lot:0031\n",
            "Write this in your check's memo line: BBL 2-02333-0031\n",
            "5536  20233300310  0000000000  250401  1  2025  8How much do I owe?\n",
            "Outstanding charges $0.00\n",
            "New charges $0.00\n",
            "Total amount due by April 1, 2025 $0.00\n",
            "Make checks payable & mail payment to:\n",
            "NYC Department of Finance\n",
            "PO Box 5536\n",
            "Binghamton NY  13902-5536Borough\n",
            "2Block\n",
            "02333Lot\n",
            "0031\n",
            "--------------------------------------------------\n",
            "Page 2:\n",
            "February 15, 2025\n",
            "Fae Holdings 395224R, LLC\n",
            "289Morris Avenue\n",
            "2-02333-0031\n",
            "Page2\n",
            "Billing Summary Amount\n",
            "Outstanding charges\n",
            "(Sum of unpaid balance and interest fees from billing periods)$0.00\n",
            "New charges\n",
            "(Sum of new property taxes and other charges-see below for details)$0.00\n",
            "AMOUNT DUE BY APRIL 1, 2025 $0.00\n",
            "Your property details:\n",
            "Estimated market value: $3,346,000\n",
            "Tax class: 4 -Commercial Or Industrial\n",
            "XXXHow we calculate your annual taxes:\n",
            "Billable assessed value: $1,398,370.00\n",
            "times the current tax rate: x10.7620%\n",
            "Annual property tax: $150,492.60\n",
            "Messages for You:\n",
            "Visitwww.nyc.gov/taxbill  toupdateyourmailingaddress,registertoreceivepropertytaxreceiptsbyemail,orlearnabout\n",
            "the interest rate charged on late payments.\n",
            "Homebankingpayment instructions: Logintoyourbankorbillpaywebsiteandadd\"NYCDepartment ofFinance\" as\n",
            "thepayee.YouraccountnumberisyourBBLnumber:2023330031.OuraddressisPOBox5536,Binghamton, NY\n",
            "13902-5536.\n",
            "Whenyouprovideacheckaspayment, youauthorize useithertouseinformation fromyourchecktomakeaone-time\n",
            "electronic fund transfer from your account or to process the payment as a check transaction.\n",
            "--------------------------------------------------\n",
            "Page 3:\n",
            "February 15, 2025\n",
            "Fae Holdings 395224R, LLC\n",
            "289Morris Avenue\n",
            "2-02333-0031\n",
            "Page3\n",
            "Additional Messages for You:\n",
            "Ifyouownincome-producing property, youmustfileaRealPropertyIncomeandExpense (RPIE)statement oraclaimof\n",
            "exclusion unlessyouareexemptbylaw.Youmustalsofileinformation aboutanygroundorsecondfloorstorefront units\n",
            "onthepremises, evenifyouareexemptfromfilinganRPIEstatement. RPIEfilerswhoseproperties haveanactual\n",
            "assessed valueof$750,000 orgreaterwillberequiredtofilerentrollinformation. ThedeadlinetofileisJune2,2025.\n",
            "Failuretofilewillresultinpenalties andinterest,whichwillbecomealienonyourpropertyiftheygounpaid.Visit\n",
            "www.nyc.gov/rpie  for more information.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data into a DataFrame\n",
        "df = pd.read_csv(\"/content/subset_041125_update.csv\")\n",
        "\n",
        "# Display results\n",
        "print(df[['BoroCode', 'Block', 'Lot', 'BBL']])\n",
        "\n",
        "bbl_list = df['BBL'].to_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwLPyNDQ7uUH",
        "outputId": "688f803a-a66e-429f-c8d3-fde53af9e4bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      BoroCode  Block  Lot         BBL\n",
            "0            2   2260    1  2022600001\n",
            "1            2   2260    4  2022600004\n",
            "2            2   2260   34  2022600034\n",
            "3            2   2261    3  2022610003\n",
            "4            2   2277    1  2022770001\n",
            "...        ...    ...  ...         ...\n",
            "2170         4  15008    8  4150080008\n",
            "2171         4  15008   33  4150080033\n",
            "2172         4  15009   25  4150090025\n",
            "2173         4  15009   51  4150090051\n",
            "2174         4  15012    6  4150120006\n",
            "\n",
            "[2175 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_text(url, max_retries=10, initial_delay=2):\n",
        "    \"\"\"\n",
        "    Download a PDF from a URL and extract text lines from the first page, with retries on connection errors.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the PDF to download.\n",
        "        max_retries (int): Maximum number of retry attempts (default: 10).\n",
        "        initial_delay (int): Initial delay in seconds before retrying (default: 2).\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted text lines if successful, or an error message string if failed.\n",
        "    \"\"\"\n",
        "    delay = initial_delay\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Attempt to download the PDF\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()  # Raises an exception for HTTP errors (e.g., 404, 500)\n",
        "\n",
        "            # Verify the content is a PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' not in content_type.lower():\n",
        "                return f\"Error: URL does not point to a PDF file (Content-Type: {content_type})\"\n",
        "\n",
        "            # Verify content starts with %PDF\n",
        "            if not response.content.startswith(b'%PDF'):\n",
        "                return \"Error: Response content is not a valid PDF\"\n",
        "\n",
        "            # Try extracting text with pdfplumber first\n",
        "            try:\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.filterwarnings(\"ignore\", category=UserWarning)  # Suppress CropBox warnings\n",
        "                    pdf_file = io.BytesIO(response.content)\n",
        "                    with pdfplumber.open(pdf_file) as pdf:\n",
        "                        first_page = pdf.pages[0]\n",
        "                        text = first_page.extract_text()\n",
        "                        if not text:\n",
        "                            return \"Error: No text extracted from the PDF\"\n",
        "                        return text.split('\\n')\n",
        "            except Exception as e:\n",
        "                # Fallback to pymupdf if pdfplumber fails\n",
        "                try:\n",
        "                    pdf_file = io.BytesIO(response.content)\n",
        "                    with pymupdf.open(stream=pdf_file, filetype=\"pdf\") as doc:\n",
        "                        first_page = doc[0]\n",
        "                        text = first_page.get_text(\"text\")\n",
        "                        if not text:\n",
        "                            return \"Error: No text extracted from the PDF using pymupdf\"\n",
        "                        return text.split('\\n')\n",
        "                except Exception as fallback_e:\n",
        "                    return f\"Error extracting text: pdfplumber failed with '{e}', pymupdf failed with '{fallback_e}'\"\n",
        "\n",
        "        except requests.ConnectionError as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Connection error: {e}. Retrying in {delay} seconds...\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2  # Exponential backoff\n",
        "            else:\n",
        "                return f\"Error downloading PDF after {max_retries} attempts: {e}\"\n",
        "        except requests.HTTPError as e:\n",
        "            return f\"HTTP Error: {e}\"\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error downloading PDF: {e}\"\n",
        "\n",
        "def extract_address(lines):\n",
        "    \"\"\"\n",
        "    Extract the mailing address from a list of text lines, starting after the line with two '#' symbols.\n",
        "\n",
        "    Args:\n",
        "        lines (list): List of text lines extracted from the PDF.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned mailing address, or an error message if extraction fails.\n",
        "    \"\"\"\n",
        "    # Find the line with two '#' symbols\n",
        "    hash_line_index = -1\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.count('#') >= 2 and re.match(r'^#.*#$', line.strip()):\n",
        "            hash_line_index = i\n",
        "            break\n",
        "\n",
        "    if hash_line_index == -1:\n",
        "        return \"Address not found: No line with two '#' symbols\"\n",
        "\n",
        "    # Collect address lines until a terminator (e.g., \"Borough:\") is found\n",
        "    address_lines = []\n",
        "    for line in lines[hash_line_index + 1:]:\n",
        "        if line.strip().startswith(\"Borough:\"):\n",
        "            break\n",
        "        if line.strip():  # Skip empty lines\n",
        "            address_lines.append(line.strip())\n",
        "\n",
        "    if not address_lines:\n",
        "        return \"Address not found: No address lines after the hash line\"\n",
        "\n",
        "    # Clean the last line to isolate city, state, and zip\n",
        "    last_line = address_lines[-1]\n",
        "    # Pattern: City (anything before) followed by ST ZIP (optional space, extra text after)\n",
        "    match = re.search(r'^(.*?)\\s+([A-Z]{2}\\s*\\d{5}(-\\d{4})?)\\s*(.*)$', last_line)\n",
        "    if match:\n",
        "        city = match.group(1).strip()\n",
        "        state_zip = match.group(2)\n",
        "        # Separate state and zip if no space exists (e.g., NY11560 -> NY 11560)\n",
        "        state_zip_match = re.match(r'([A-Z]{2})(\\d{5}(-\\d{4})?)', state_zip)\n",
        "        if state_zip_match:\n",
        "            state = state_zip_match.group(1)\n",
        "            zip_code = state_zip_match.group(2)\n",
        "            cleaned_last_line = f\"{city} {state} {zip_code}\"\n",
        "            address_lines[-1] = cleaned_last_line\n",
        "\n",
        "    return '\\n'.join(address_lines)\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Base URL template\n",
        "url_template = 'https://a836-edms.nyc.gov/dctm-rest/repositories/dofedmspts/StatementSearch?bbl={}&stmtDate=20250215&stmtType=SOA'"
      ],
      "metadata": {
        "id": "Pd4nOR92FTlo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress all pdfminer warnings by redirecting its logging\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "\n",
        "# Set up logging for our application\n",
        "logging.basicConfig(\n",
        "    filename='bbl_processing.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def download_and_extract_text(url, max_retries=3, initial_delay=1):\n",
        "    \"\"\"\n",
        "    Download a PDF from a URL and extract text lines from the first page, with retries on connection errors.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the PDF to download.\n",
        "        max_retries (int): Maximum number of retry attempts (default: 3).\n",
        "        initial_delay (int): Initial delay in seconds before retrying (default: 1).\n",
        "\n",
        "    Returns:\n",
        "        list: Extracted text lines if successful, or an error message string if failed.\n",
        "    \"\"\"\n",
        "    delay = initial_delay\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Attempt to download the PDF\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Verify the content is a PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' not in content_type.lower():\n",
        "                return f\"Error: URL does not point to a PDF file (Content-Type: {content_type})\"\n",
        "\n",
        "            # Verify content starts with %PDF\n",
        "            if not response.content.startswith(b'%PDF'):\n",
        "                return \"Error: Response content is not a valid PDF\"\n",
        "\n",
        "            # Try extracting text with pdfplumber first\n",
        "            try:\n",
        "                with warnings.catch_warnings():\n",
        "                    warnings.simplefilter(\"ignore\")  # Additional safety\n",
        "                    pdf_file = io.BytesIO(response.content)\n",
        "                    with pdfplumber.open(pdf_file) as pdf:\n",
        "                        first_page = pdf.pages[0]\n",
        "                        text = first_page.extract_text()\n",
        "                        if not text:\n",
        "                            return \"Error: No text extracted from the PDF\"\n",
        "                        return text.split('\\n')\n",
        "            except Exception as e:\n",
        "                # Fallback to pymupdf\n",
        "                try:\n",
        "                    pdf_file = io.BytesIO(response.content)\n",
        "                    with pymupdf.open(stream=pdf_file, filetype=\"pdf\") as doc:\n",
        "                        first_page = doc[0]\n",
        "                        text = first_page.get_text(\"text\")\n",
        "                        if not text:\n",
        "                            return \"Error: No text extracted from the PDF using pymupdf\"\n",
        "                        return text.split('\\n')\n",
        "                except Exception as fallback_e:\n",
        "                    return f\"Error extracting text: pdfplumber failed with '{e}', pymupdf failed with '{fallback_e}'\"\n",
        "\n",
        "        except requests.ConnectionError as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                logging.warning(f\"Connection error for {url}: {e}. Retrying in {delay}s...\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "            else:\n",
        "                return f\"Error downloading PDF after {max_retries} attempts: {e}\"\n",
        "        except requests.HTTPError as e:\n",
        "            return f\"HTTP Error: {e}\"\n",
        "        except requests.RequestException as e:\n",
        "            return f\"Error downloading PDF: {e}\"\n",
        "\n",
        "def extract_address(lines):\n",
        "    \"\"\"Extract the mailing address starting after the line with two '#' symbols.\"\"\"\n",
        "    hash_line_index = -1\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.count('#') >= 2 and re.match(r'^#.*#$', line.strip()):\n",
        "            hash_line_index = i\n",
        "            break\n",
        "\n",
        "    if hash_line_index == -1 or hash_line_index + 1 >= len(lines):\n",
        "        return \"Address not found: No line with two '#' symbols or insufficient lines follow\"\n",
        "\n",
        "    address_lines = []\n",
        "    start_index = hash_line_index + 1\n",
        "\n",
        "    if start_index < len(lines):\n",
        "        line = lines[start_index].replace(\"Make checks payable & mail payment to:\", \"\").strip()\n",
        "        address_lines.append(line)\n",
        "\n",
        "    if start_index + 1 < len(lines):\n",
        "        line = lines[start_index + 1].replace(\"NYC Department of Finance\", \"\").strip()\n",
        "        address_lines.append(line)\n",
        "\n",
        "    if start_index + 2 < len(lines):\n",
        "        line = lines[start_index + 2].strip()\n",
        "        address_lines.append(line)\n",
        "\n",
        "    if start_index + 4 < len(lines):\n",
        "        line = lines[start_index + 4].replace(\"Binghamton NY 13902-5536\", \"\").strip()\n",
        "        address_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(address_lines)\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Process each BBL with a clean progress bar\n",
        "for bbl in tqdm(bbl_list, desc=\"Processing BBLs\", file=sys.stdout):\n",
        "    try:\n",
        "        url = url_template.format(bbl)\n",
        "        logging.info(f\"Processing BBL {bbl}: {url}\")\n",
        "        text_lines = download_and_extract_text(url, max_retries=3, initial_delay=1)\n",
        "        if isinstance(text_lines, list):\n",
        "            address = extract_address(text_lines)\n",
        "            results[bbl] = address\n",
        "            logging.info(f\"Success for BBL {bbl}: {address}\")\n",
        "        else:\n",
        "            results[bbl] = text_lines\n",
        "            logging.warning(f\"Failed for BBL {bbl}: {text_lines}\")\n",
        "        time.sleep(0.5)  # Rate limiting\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Unexpected error for BBL {bbl}: {str(e)}\"\n",
        "        results[bbl] = error_msg\n",
        "        logging.error(error_msg)\n",
        "\n",
        "# Save results to a file\n",
        "with open('bbl_results.txt', 'w') as f:\n",
        "    for bbl, address in results.items():\n",
        "        f.write(f\"BBL: {bbl}\\nAddress:\\n{address}\\n{'-'*40}\\n\")\n"
      ],
      "metadata": {
        "id": "GC2Wp60f8Vqm",
        "outputId": "23c9f48c-6412-4c8a-8714-84acceb908bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BBLs: 100%|██████████| 5/5 [00:07<00:00,  1.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join results with data and output a new csv\n",
        "\n",
        "results_df = pd.DataFrame(list(results.items()), columns=['BBL', 'Address'])\n",
        "\n",
        "df['BBL'] = df['BBL'].astype(str)\n",
        "results_df['BBL'] = results_df['BBL'].astype(str)\n",
        "\n",
        "merged_df = pd.merge(df, results_df, on='BBL', how='left')\n",
        "\n",
        "merged_df['Address_y'] = merged_df['Address_y'].str.replace(\n",
        "    r'\\nBorough: \\d+\\s+Block: \\d+\\s+Lot: \\d+$',\n",
        "    '',\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "merged_df.to_csv(\"MapPluto_Subset_with_Mailing_Addr.csv\")"
      ],
      "metadata": {
        "id": "xrDKEn6rXSzv"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}